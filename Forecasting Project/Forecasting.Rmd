---
title: "Comparing Auto-ARIMA to NNAR"
author: "Luke Harold"
date: "2025-09-10"
output:
  pdf_document:
    latex_engine: pdflatex
fontsize: 11pt
geometry: margin=1in
---

```{r setup, include=FALSE}
# Set CRAN mirror for non-interactive knitting
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Global chunk defaults
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE,
  fig.align = "center", cache = TRUE, autodep = TRUE
)

set.seed(1234)

# Install/load (only runs if pacman is missing)
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
pacman::p_load(
  tidyverse, forecast, tsibble, gridExtra, timetk, tidymodels,
  ranger, glmnet, zoo, fable, feasts, future.apply, lubridate
)

source("scripts/Forecasting_functions.R", local = knitr::knit_global())
```
## Introduction

The goal of this assignment was to build a model to forecast the unemployment rate for
an assigned U.S. state, in this case, Maryland. The training window ranged from 1980 to
December 2012, with the goal of producing a recursive forecast from 2013 onward. There
are several important considerations in constructing an accurate forecasting model. This
paper focuses on comparing three traditional time series models using recursive forecasting,
followed by an attempt to use a Neural Network Autoregression (NNAR) model with an
adaptive training window to see whether it can outperform the classic models. The goal
was to determine whether the NNAR model can better account for structural features of the
time series, such as cyclicality and time-varying seasonality.

#loading data
```{r data, echo=FALSE}
Maryland_unemployment <- read_csv("Data/Maryland_unemployment_rate.csv") %>%
  arrange(date)  # ensure chronological

# Plot raw unemployment rate
Maryland_unemployment %>%
  ggplot(aes(x = date, y = value)) +
  geom_line() +
  labs(title = "Unemployment Rate in Maryland", x = "", y = "Unemployment Rate") +
  theme_minimal()

# tsibble
Maryland <- Maryland_unemployment %>%
  mutate(date = yearmonth(date)) %>%
  as_tsibble(index = date)

# base ts (monthly) from NUMERIC vector
start_year  <- as.integer(format(min(Maryland_unemployment$date), "%Y"))
start_month <- as.integer(format(min(Maryland_unemployment$date), "%m"))

vals <- dplyr::pull(Maryland_unemployment, value)

# Optional: warn if log() will break
if (any(vals <= 0, na.rm = TRUE)) {
  warning("Non-positive values detected; log() will produce -Inf/NA.")
}

Maryland_ts <- ts(log(vals), start = c(start_year, start_month), frequency = 12)
```

## Elements of the time series
Visualy the data exhibits a few key features including cylclicallity, seasnality and a downward trend. In order to combat this we can apply a log transformation to stabilize the variance and then decompose the series to better understand its components. By applying a log transformation, we can stabilize the variance and make the series more stationary, which is often a prerequisite for many time series forecasting methods. As we will be using Auto Arima to choose a model to compare with the Neural Network Autoregression (NNAR) model, we won't pre difference the data.


```{r, echo=FALSE}
#apply log transformation and plot the data
Maryland_ts_log <- Maryland %>%
  mutate(log_value = log(value))
Maryland_ts_log %>%
  ggplot(aes(x = date, y = log_value)) +
  geom_line() +
  labs(title = "Log Transformed Unemployment Rate in Maryland",
       x = "", y = "Log(Unemployment Rate)") +
  theme_minimal()
```
#investigating the changing seasonality and structural breaks
Given that the there is seasonality in the data, it is interested to visualize whether this component remains constant over time or if these trends change. With an individual state's unemployment rate it is possible that between 1980-2024 the industry composition of the state has changed, which could lead to changes in seasonal patterns. To investigate this we can use a seasonal subseries plot to visualize the seasonality over time. 

Including the covid period may be more indicative of when restrictions were put in place and the unemployment rate spiked, but it is also a structural break in the data that may not be representative of future trends. If we deconstruct the unemployment rate into 20 year periods we see that the seasonal patterns appear to change slightly.
```{r, echo=FALSE}
Maryland_ts_log %>%
  gg_season(log_value, labels = "both") +
  labs(title = "Seasonal Plot of Log Transformed Unemployment Rate in Maryland",
       x = "Month", y = "Log(Unemployment Rate)") +
  theme_minimal()
Maryland_ts_log %>% 
  gg_subseries(log_value)
#decomposing the series
dcmp<-Maryland_ts_log %>%
  model(STL(log_value))
components(dcmp) %>%
  autoplot() +
  labs(title = "STL Decomposition of Log Transformed Unemployment Rate in Maryland",
       x = "", y = "Log(Unemployment Rate)") +
  theme_minimal()
```

```{r, echo=FALSE}
#fitting models
maryland_train <- Maryland_ts_log %>%
  filter_index("1980 Jan" ~ "2012 Dec") %>%
  rename(log_unemploy = log_value)
fit <- maryland_train %>%
  model(
    AR2 = ARIMA(log_unemploy ~ pdq(2,1,0)),
    auto = ARIMA(log_unemploy, stepwise = FALSE, approx = FALSE)
  )
fit %>%  pivot_longer(everything(), names_to = "Model name",
                    values_to = "Orders")
glance(fit) %>%  
  arrange(AICc) %>% 
  select(.model:BIC)
```

#Using auto arima and a manual ARIMA(2,1,0) to compare with NNAR we determine that the model selected by auto arima to have residuals that are white noise whereas the residuals from the AR2 are not. Evidence from the ljung box test shows that the p-value for the auto arima model is above 0.05, indicating that we fail to reject the null hypothesis that the residuals are white noise. In contrast, the AR2 model has a p-value below 0.05, suggesting that its residuals are not white noise and that there may be some autocorrelation present.

```{r, echo=FALSE}
fit %>%
  select(auto) %>%
  gg_tsresiduals()
augment(fit) %>%
  filter(.model == "auto") %>%
  features(.innov, ljung_box, lag = 24, dof = 6) 

fit %>%
  select(auto) %>%
  report()
#same for AR2
fit %>%
  select(AR2) %>%
  gg_tsresiduals()
augment(fit) %>%
  filter(.model == "AR2") %>%
  features(.innov, ljung_box, lag = 24, dof = 3)
fit %>%
  select(AR2) %>%
  report()

```


```{r model_run, echo=FALSE, cache=TRUE}
knitr::opts_current$set(
  dependson   = "data",
  cache.extra = list(file.mtime("scripts/Forecasting_functions.R"),
                     file.mtime("Data/Maryland_unemployment_rate.csv"),
                     1234)
)

auto_arima <- function(x) {
  Arima(x, order = c(2,0,2),
        seasonal = list(order = c(0,1,2), period = frequency(x)),
        include.mean = FALSE)
}

AR2 <- function(x) {
  Arima(x, order = c(2,1,0),
        seasonal = list(order = c(0,1,1), period = frequency(x)),
        include.mean = FALSE)
}


#function that applies recursive forecasting to multiple models, see scripts/Forecasting_functions.R for details
start <- Sys.time()
res2 <- recursive_forecast_multi(Maryland_ts,
                                 start_train = c(1980,1),
                                 end_train   = c(2012,12),
                                 model_funs  = list(forecast::naive, auto_arima, AR2),
                                 names       = c("Naive", "autoARIMA", "AR2"),
                                 h = 1)
head(res2)
end <- Sys.time(); end - start
#calculate the rmse
res2<-res2 %>%
  mutate(
    Actual_level = exp(Actual),
    Auto_level   = exp(autoARIMA),
    AR2_level    = exp(AR2),
    Auto_fit     = Actual_level - Auto_level,
    AR2_fit      = Actual_level - AR2_level,
    
  )
res2
#calcualte rmse
rmse_AR2 <- sqrt(mean(res2$AR2_fit^2))
rmse_auto <- sqrt(mean(res2$Auto_fit^2))
performance<-cbind(rmse_AR2, rmse_auto)
performance
#rm(performance)
```

```{r, echo=FALSE}
#attach the level scale forecasts to the original data for plotting
autoplot(ts(res2[,c("Actual_level","Auto_level","AR2_level")], start=c(2013,1), frequency=12)) +
  labs(title = "Recursive 1-step forecasts from Auto ARIMA and AR2 models",
       x = "Year", y = "Unemployment Rate") +
  theme_minimal()

```
Comparing the two models we see that the auto arima model has a lower rmse than the AR2 model, but visually we can see that both struggle greatly to capture the spike in unemployment during the covid period. Is there potentially a model that can better capture these structural breaks and changing seasonal patterns? Neural Network Auto-regression (NNAR) is a forecasting method that uses a feed-forward neural network to model the relationship between past values of a time series and its future values. It is used for its ability to capture complex patterns in the data and is not constrained by the assumptions of traditional time series models.

```{r, echo=FALSE}
set.seed(1234)
fit <- Maryland_ts_log %>% 
  model(NNETAR(log_value))
forecasts_NN<-fit %>% 
  forecast(h = 60)  
fitted(fit)
glance(fit)
report(fit)
#attach forecasts to the maryland data for plotting
# Plot actual + fitted + forecasts
autoplot(forecasts_NN, Maryland_ts_log) +
  labs(title = "NNAR Forecasts for Maryland Unemployment Rate",
       x = "Year", y = "Log(Unemployment Rate)") +
  theme_minimal()
```

#Neural Networks
Neural Network Auto-regression uses several parameters including the number of lagged inputs(p), the number of seasonal lagged inputs (P), the number of nodes in the hidden layer (size), and the number of times to repeat the fitting process with different random starting weights (repeats). These parameters can be changed to see which combination fits the data best. Here we train a NNAR on the training window to find the best structure, which we will then use in the recursive forecast allowing the weights to change but keeping the 5 lags, 1 seasonal lag and 4 hidden nodes. 

```{r, echo=FALSE}
set.seed(1234)
train0 <- window(Maryland_ts, start = c(1980,1), end = c(2012,12))
fit0   <- forecast::nnetar(train0)
fit0
```
Here we see that it outperforms the arima models from earlier. 

```{r nnar_fixed_run, echo=FALSE, cache=TRUE}
knitr::opts_current$set(
  dependson   = "data",  # <- the label of your data chunk
  cache.extra = list(
    file.mtime("scripts/Forecasting_functions.R"),
    file.mtime("Data/Maryland_unemployment_rate.csv"),
    1234
  )
)
#1
train0 <- stats::window(Maryland_ts, start = c(1980,1), end = c(2012,12))
fit0   <- forecast::nnetar(train0)
fit0

# 2) Wrap those choices; this function will be called at each step
my_nnar_fixed <- function(x) {
  forecast::nnetar(x, p = fit0$p, P = fit0$P, size = fit0$size, repeats = 20)
}
start <- Sys.time()
nnforecast <- recursive_forecast_multi(
  Maryland_ts,
  start_train = c(1980,1),
  end_train   = c(2012,12),
  model_funs  = list(my_nnar_fixed),
  names       = "NNAR_fixed",
  h = 1
)
end <- Sys.time(); end - start


nnforecast <- nnforecast %>%
  mutate(
    Actual_level = exp(Actual),
    NNAR_fixed_level   = exp(NNAR_fixed),
    NNAR_fit     = Actual_level - NNAR_fixed_level
  )
rmse_nnar <- sqrt(mean(nnforecast$NNAR_fit^2))
rmse_nnar
#add rmse_nnar to performance table
performance<-cbind(performance, rmse_nnar)
performance
```

```{r nnar_adaptive, echo=FALSE, cache=TRUE}
knitr::opts_current$set(
  dependson   = "data",  # <- the label of your data chunk
  cache.extra = list(
    file.mtime("scripts/Forecasting_functions.R"),
    file.mtime("Data/Maryland_unemployment_rate.csv"),
    1234
  )
)
set.seed(1234)
# 2) Wrap those choices; this function will be called at each step
nnar_adaptive <- function(x) {
  forecast::nnetar(x, repeats = 20)
}
start <- Sys.time()
nn_adapt_forecast <- recursive_forecast_multi(
  Maryland_ts,
  start_train = c(1980,1),
  end_train   = c(2012,12),
  model_funs  = list(nnar_adaptive),
  names       = "NNAR_adaptive",
  h = 1
)
end <- Sys.time(); end - start


nn_adapt_forecast <- nn_adapt_forecast %>%
  mutate(
    Actual_level = exp(Actual),
    NNAR_adapt_level   = exp(NNAR_adaptive),
    NNAR_adaptive_fit     = Actual_level - NNAR_adapt_level
  )
rmse_nnar_adaptive <- sqrt(mean(nn_adapt_forecast$NNAR_adaptive_fit^2))
rmse_nnar_adaptive
#add rmse_nnar to performance table
performance<-cbind(performance, rmse_nnar_adaptive)
performance
```

```{r nnar_dynamic, echo=FALSE, cache=TRUE}
knitr::opts_current$set(
  dependson   = "data",  # <- the label of your data chunk
  cache.extra = list(
    file.mtime("scripts/Forecasting_functions.R"),
    file.mtime("Data/Maryland_unemployment_rate.csv"),
    1234
  )
)
# 2) Run dynamic-window NNAR (1-year window steps, min 3y, no max cap)
start <- Sys.time()
res_dyn <- dynamic_nnar_greedy(
  ts_data     = Maryland_ts,         # your LOG series, if that's what you're using
  start_train = c(1980,1),
  end_train   = c(2012,12),
  p = 5, P = 1, size = 4, repeats = 20,
  h = 1,
  min_years   = 3,
  step_years  = 1,
  max_years   = NULL,
  lambda      = NULL,                # keep NULL if input is already logged
  name        = "NNAR_dyn"
)
end <- Sys.time(); end - start

res_dyn <- res_dyn %>%
  mutate(
    Actual_level = exp(Actual),
    NNAR_dyn_level   = exp(NNAR_dyn),
    NNAR_dyn_fit     = Actual_level - NNAR_dyn_level
  )
res_dyn<-res_dyn %>% 
  select(-time)
rmse_nnar_dyn <- sqrt(mean(res_dyn$NNAR_dyn_fit^2))
rmse_nnar_dyn
performance<-cbind(performance, rmse_nnar_dyn)
performance
```



```{r, echo=FALSE}
#attach all three NNAR forecasts to the original data for plotting
#res_dyn, nnforecast, nn_adapt_forecast, attach by actual
if (exists("all_nnar")) rm(all_nnar)

all_nnar <- dplyr::bind_cols(
  nnforecast        %>% dplyr::select(Actual_level, NNAR_fixed_level),
  nn_adapt_forecast %>% dplyr::select(NNAR_adapt_level),
  res_dyn           %>% dplyr::select(NNAR_dyn_level)
)

autoplot(ts(all_nnar[, c("Actual_level","NNAR_fixed_level","NNAR_adapt_level","NNAR_dyn_level")],
            start = c(2013,1), frequency = 12)) +
  labs(title = "Recursive 1-step forecasts from NNAR models",
       x = "Year", y = "Unemployment Rate") +
  theme_minimal()

```


```{r, echo=FALSE}
#pretty table showing the rmse for all models

performance %>%
  knitr::kable(caption = "RMSE for all models")
```
#NNAR model without the adaptive window

Neural Network Autoregression (NNAR) is a type of time series forecasting that trains a feedforward neural network using lagged values of the time series as inputs.  
